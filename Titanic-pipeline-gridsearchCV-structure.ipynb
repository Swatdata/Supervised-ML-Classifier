{"cells":[{"cell_type":"markdown","source":["# Titanic 3: Creating a Pipeline and tuning the model with Grid Search Cross Validation"],"metadata":{"id":"qXFGScYJv0zO"}},{"cell_type":"markdown","source":["## 1. Data reading and preprocessing\n","\n","We will first review everything we did in the previous notebook."],"metadata":{"id":"hvblkIsDwBlI"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"fgUcuPKovu5T","executionInfo":{"status":"ok","timestamp":1666790506158,"user_tz":-120,"elapsed":1945,"user":{"displayName":"swathy s","userId":"09634650848315366473"}}},"outputs":[],"source":["import pandas as pd\n","\n","url = \"https://drive.google.com/file/d/1g3uhw_y3tboRm2eYDPfUzXXsw8IOYDCy/view?usp=sharing\"\n","path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n","\n","data = pd.read_csv(path)"]},{"cell_type":"markdown","metadata":{"id":"57d0l4uEvu5W"},"source":["### 1.1. Setting X and y\n","\n","- **X**: columns that help us make a prediction.\n","- **y**: the column that we want to predict."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"3rX2vgy5vu5Y","executionInfo":{"status":"ok","timestamp":1666790506158,"user_tz":-120,"elapsed":4,"user":{"displayName":"swathy s","userId":"09634650848315366473"}}},"outputs":[],"source":["X = data.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\"])\n","y = X.pop(\"Survived\")"]},{"cell_type":"markdown","metadata":{"id":"9kP78izKvu5Y"},"source":["### 1.2. Feature Selection: keeping only numerical features\n","\n","Scikit-Learn models cannot deal with categorical features"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"JgVzbmBGvu5Z","executionInfo":{"status":"ok","timestamp":1666790506159,"user_tz":-120,"elapsed":4,"user":{"displayName":"swathy s","userId":"09634650848315366473"}}},"outputs":[],"source":["X_num = X.select_dtypes(include=\"number\")"]},{"cell_type":"markdown","metadata":{"id":"IDfzSb5hvu5Z"},"source":["### 1.3. Data splitting"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"ul_C6DXMvu5a","executionInfo":{"status":"ok","timestamp":1666790506159,"user_tz":-120,"elapsed":4,"user":{"displayName":"swathy s","userId":"09634650848315366473"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_num_train, X_num_test, y_train, y_test = train_test_split(X_num, \n","                                                            y, \n","                                                            test_size=0.2, \n","                                                            random_state=123)"]},{"cell_type":"markdown","metadata":{"id":"x9RPgKLhvu5b"},"source":["### 1.4. Imputing missing values\n","\n","(Fit on train, transform train & test)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"a6KZkxN3vu5b","executionInfo":{"status":"ok","timestamp":1666790506773,"user_tz":-120,"elapsed":618,"user":{"displayName":"swathy s","userId":"09634650848315366473"}}},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","\n","my_imputer = SimpleImputer() # initialize\n","my_imputer.fit(X_num_train) # fit on the train set\n","X_num_imputed_train = my_imputer.transform(X_num_train) # transform the train set\n","X_num_imputed_test = my_imputer.transform(X_num_test) # transform the test set"]},{"cell_type":"markdown","metadata":{"id":"g0QFXBvGvu5c"},"source":["### 1.5. Modelling: Decision Tree\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"oZmmxJ63vu5d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666790506773,"user_tz":-120,"elapsed":14,"user":{"displayName":"swathy s","userId":"09634650848315366473"}},"outputId":"90af5aac-ae2f-44b5-dd54-e72b133a3c7e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DecisionTreeClassifier(max_depth=4, min_samples_leaf=10)"]},"metadata":{},"execution_count":19}],"source":["# 1. import the model\n","from sklearn.tree import DecisionTreeClassifier \n","\n","# 2. initialize the model\n","my_tree = DecisionTreeClassifier(max_depth=4,\n","                                 min_samples_leaf=10\n","                                )\n","\n","# 3. fit the model to the train data\n","my_tree.fit(X = X_num_imputed_train, \n","            y = y_train)"]},{"cell_type":"markdown","metadata":{"id":"mySTGdw_vu5d"},"source":["### 1.6. Check accuracy on the train set"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"kfa0Aoi6vu5e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666790506774,"user_tz":-120,"elapsed":12,"user":{"displayName":"swathy s","userId":"09634650848315366473"}},"outputId":"5cb75b5a-fb2c-4685-c6dd-d40e9bdfdff3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7191011235955056"]},"metadata":{},"execution_count":20}],"source":["from sklearn.metrics import accuracy_score\n","\n","y_pred_tree_train = my_tree.predict(X_num_imputed_train)\n","\n","accuracy_score(y_true = y_train,\n","               y_pred = y_pred_tree_train)"]},{"cell_type":"markdown","metadata":{"id":"uAUerP3Gvu5e"},"source":["### 1.7. Check accuracy on the test set"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"P3DZqJnnvu5e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666790506774,"user_tz":-120,"elapsed":9,"user":{"displayName":"swathy s","userId":"09634650848315366473"}},"outputId":"874bf48f-b950-4541-b5fe-dd0fff8019bb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7541899441340782"]},"metadata":{},"execution_count":21}],"source":["y_pred_tree_test = my_tree.predict(X_num_imputed_test)\n","\n","accuracy_score(y_true = y_test,\n","               y_pred = y_pred_tree_test)"]},{"cell_type":"markdown","metadata":{"id":"Rm08l1JBvu5h"},"source":["## 2. Creating a Pipeline\n","\n","Before moving forward in our quest to improve the model, take a moment to learn how to use Scikit-Learn Pipelines. They will not increase your performance, but they are a necessary tool to compress all the steps in the data preparation + modelling phases into a single one, and this will become very relevant as we move forward and keep adding new steps:\n","\n","* Read the lesson \"Scikit-Learn Pipelines\" on the platform.\n","\n","* Check the docs: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"kdkaTOjYvu5h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666790506774,"user_tz":-120,"elapsed":7,"user":{"displayName":"swathy s","userId":"09634650848315366473"}},"outputId":"9abff651-f8a7-4857-a983-54704e99424f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n","       0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n","       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n","       0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n","       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n","       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n","       0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n","       1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n","       1, 0, 0])"]},"metadata":{},"execution_count":22}],"source":["from sklearn.impute import SimpleImputer\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.pipeline import make_pipeline\n","\n","# 1. initialize transformers &amp; model\n","imputer = SimpleImputer(strategy=\"median\")\n","dtree = DecisionTreeClassifier(max_depth=4,\n","                               min_samples_leaf=10)\n"," \n","# 2. Create a pipeline*\n","pipe = make_pipeline(imputer, dtree)\n"," \n","# 3. Fit the pipeline to the training data\n","pipe.fit(X_num_train, y_train)\n"," \n","# 4. Use the pipeline to make predictions\n","pipe.predict(X_num_test)"]},{"cell_type":"markdown","source":["Now, the object `pipie` can take (almost) raw data as input and output predictions. We no longer need to impute missing values and use the model to make predictions in separate steps."],"metadata":{"id":"oXN7nfdoa6IT"}},{"cell_type":"markdown","metadata":{"id":"EuOESLyyvu5i"},"source":["## 3. Using GridsearchCV to find the best parameters\n","\n","So far, we tuned the hyperparameters of the decision tree manually. This is not ideal, for two reasons:\n","\n","- It's not efficient in terms of quickly finding the best combination of parameters.\n","- If we keep checking the performance on the test set over and over again, we might end up creating a model that fits that particular test set, but does not generalize as well with new data. Test sets are meant to reamain unseen until the very last moment of ML development â€”we have been cheating a bit!\n","\n","Grid Search Cross Validation solves both issues:\n","\n","* Read the lesson \"Housing Prices: Iteration 2, Grid Search & Cross Validation\" on the platform.\n","\n","* Check the docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"W7awfF-qvu5i","executionInfo":{"status":"ok","timestamp":1666790506774,"user_tz":-120,"elapsed":6,"user":{"displayName":"swathy s","userId":"09634650848315366473"}}},"outputs":[],"source":["# 1. initialize transformers &amp; model\n","imputer = SimpleImputer()\n","dtree = DecisionTreeClassifier()\n"," \n","# 2. Create a pipeline*\n","pipe = make_pipeline(imputer, dtree)\n","\n","param_grid = {\n","    'decisiontreeclassifier__max_depth': range(2, 12),\n","    'decisiontreeclassifier__min_samples_leaf': range(3, 10, 2),\n","    'decisiontreeclassifier__min_samples_split': range(3, 40, 5),\n","    'decisiontreeclassifier__criterion':['gini', 'entropy']\n","    }\n","    \n","from sklearn.model_selection import GridSearchCV\n"," \n","search = GridSearchCV(pipe, # you have defined this beforehand\n","                      param_grid, # your parameter grid\n","                      cv=5, # the value for K in K-fold Cross Validation\n","                      scoring='accuracy', # the performance metric to use, \n","                      verbose=1) # we want informative outputs during the training process"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"aHJ_JHWEvu5i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666790529893,"user_tz":-120,"elapsed":23124,"user":{"displayName":"swathy s","userId":"09634650848315366473"}},"outputId":"70aa1155-ca15-440e-8212-692631b77bce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 640 candidates, totalling 3200 fits\n"]},{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=5,\n","             estimator=Pipeline(steps=[('simpleimputer', SimpleImputer()),\n","                                       ('decisiontreeclassifier',\n","                                        DecisionTreeClassifier())]),\n","             param_grid={'decisiontreeclassifier__criterion': ['gini',\n","                                                               'entropy'],\n","                         'decisiontreeclassifier__max_depth': range(2, 12),\n","                         'decisiontreeclassifier__min_samples_leaf': range(3, 10, 2),\n","                         'decisiontreeclassifier__min_samples_split': range(3, 40, 5)},\n","             scoring='accuracy', verbose=1)"]},"metadata":{},"execution_count":24}],"source":["search.fit(X_num_imputed_train, y_train)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"UhJHlajCvu5i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666790529893,"user_tz":-120,"elapsed":9,"user":{"displayName":"swathy s","userId":"09634650848315366473"}},"outputId":"27371c5a-d930-4c9b-a901-39519c974b21"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'decisiontreeclassifier__criterion': 'gini',\n"," 'decisiontreeclassifier__max_depth': 6,\n"," 'decisiontreeclassifier__min_samples_leaf': 9,\n"," 'decisiontreeclassifier__min_samples_split': 3}"]},"metadata":{},"execution_count":25}],"source":["search.best_params_"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"UzXybXmDvu5i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666790529894,"user_tz":-120,"elapsed":9,"user":{"displayName":"swathy s","userId":"09634650848315366473"}},"outputId":"1498b2c4-ded7-4644-cf60-fde9774719fe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7023047375160052"]},"metadata":{},"execution_count":26}],"source":["search.best_score_"]},{"cell_type":"markdown","source":["## **Your challenge**\n","\n","In a new notebook, apply everything you have learned here to the Housing project, following the Learning platform when needed."],"metadata":{"id":"U5zAyC8_Uwpz"}},{"cell_type":"code","source":[],"metadata":{"id":"3PMad0HLXeZO","executionInfo":{"status":"ok","timestamp":1666790529894,"user_tz":-120,"elapsed":7,"user":{"displayName":"swathy s","userId":"09634650848315366473"}}},"execution_count":26,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"1b_937Zoc2h8S8mo6My2sg6XwBc9I3T2U","timestamp":1666785265743}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}